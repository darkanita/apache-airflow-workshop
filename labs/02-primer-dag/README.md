# Lab 02 â€“ Crear un DAG con dependencias (ETL simple)

En este laboratorio escribirÃ¡s un **DAG con 3 tareas encadenadas** que simulan un flujo **ETL**:
1) `extract` â†’ genera datos de ventas (CSV)
2) `transform` â†’ calcula mÃ©tricas (JSON)
3) `load` â†’ 'carga' los datos (simulado) y registra el resultado

> Este lab se enfoca en **definir dependencias** entre tareas con Airflow 2.x.

## ğŸ¯ Objetivos
- Definir mÃºltiples tareas en un DAG.
- Encadenar tareas con `>>` (o `set_downstream`).
- Entender el patrÃ³n **ETL** dentro de Airflow.

## ğŸ“‚ Estructura
```
labs/02-primer-dag/
â”œâ”€â”€ README.md
â””â”€â”€ dags/
    â””â”€â”€ etl_dag.py
```

## â–¶ï¸ Pasos
1. Copia esta carpeta dentro de tu repo del workshop.
2. AsegÃºrate de tener corriendo el stack de Airflow (Lab 01):
   ```bash
   docker compose up
   ```
3. Entra a la UI: http://localhost:8080
4. Activa el DAG **etl_dag** y lanza una ejecuciÃ³n manual (Trigger DAG).

## ğŸ” Â¿QuÃ© hace cada tarea?
- **extract**: crea `/opt/airflow/dags/data/sales_YYYYMMDD.csv` con datos ficticios.
- **transform**: lee el CSV, calcula total y promedio y genera `/opt/airflow/dags/data/metrics_YYYYMMDD.json`.
- **load**: simula cargar las mÃ©tricas a un sistema destino (solo imprime en logs).

## ğŸ§¹ Limpieza
Los archivos se escriben bajo `dags/data/`. Puedes borrarlos si deseas reiniciar el ejercicio.

---

âœ… Consejo: Abre el DAG â†’ pestaÃ±a **Graph** para ver el flujo ETL, ejecuta y revisa **Logs** de cada tarea.
